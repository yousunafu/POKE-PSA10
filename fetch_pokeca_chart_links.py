"""
ã¿ã‚“ãªã®ãƒã‚±ã‚«ç›¸å ´ã®ã‚«ãƒ¼ãƒ‰è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

filtered_cards.csv ã® card_number ã‚’ã‚‚ã¨ã«æ¤œç´¢ã—ã€
å„ã‚«ãƒ¼ãƒ‰ã® pokeca-chart.com è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’æŠ½å‡ºã—ã¦ pokeca_chart_links.json ã«ä¿å­˜ã™ã‚‹ã€‚

æ¤œç´¢çµæœãŒ JavaScript ã§é…å»¶è¡¨ç¤ºã•ã‚Œã‚‹ãŸã‚ Playwright ã‚’ä½¿ç”¨ã—ã€
è¡¨ç¤ºå¾…ã¡ã‚’å…¥ã‚Œã¦ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹ã€‚

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  --test  å…ˆé ­8ä»¶ã®ã¿å‡¦ç†
  --headed  ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤ºï¼ˆãƒœãƒƒãƒˆå¯¾ç­–ãŒå³ã—ã„å ´åˆã«è©¦ã™ï¼‰
"""
import csv
import json
import re
import sys
import time
from collections import Counter
from pathlib import Path
from urllib.parse import quote

from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
ROOT = Path(__file__).resolve().parent
CSV_PATH = ROOT / "filtered_cards.csv"
OUTPUT_PATH = ROOT / "pokeca_chart_links.json"
BASE_URL = "https://pokeca-chart.com"
SEARCH_URL = f"{BASE_URL}/"
# ã‚«ãƒ¼ãƒ‰è©³ç´°ãƒšãƒ¼ã‚¸ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³
# 1) æ¨™æº–: /s6a-093-069/, /s8a-p-001-025/ ãªã©æœ«å°¾ãŒ -æ•°å­—-æ•°å­—
CARD_LINK_PATTERN_STD = re.compile(
    rf"^{re.escape(BASE_URL)}/([a-z0-9]+(?:-[a-z0-9]+)*-\d+-\d+)/?$"
)
# 2) ç‰¹æ®Šï¼ˆ001/S-P, 001/SV-P ãªã©ï¼‰: /001-s-p/, /001-sv-p/ å½¢å¼ï¼ˆcard_number ã® / â†’ - å°æ–‡å­—åŒ–ï¼‰
CARD_LINK_PATTERN_SPECIAL = re.compile(
    rf"^{re.escape(BASE_URL)}/(\d+-[a-z0-9]+(?:-[a-z0-9]+)*)/?$"
)
REQUEST_DELAY_SEC = 1.5
PAGE_LOAD_WAIT_SEC = 5.0  # æ¤œç´¢çµæœã®è¡¨ç¤ºå¾…ã¡ï¼ˆJSé…å»¶è¡¨ç¤ºã®ãŸã‚å¤šã‚ã«ï¼‰
RESULTS_LINK_WAIT_MS = 12000  # ã‚«ãƒ¼ãƒ‰è©³ç´°ãƒªãƒ³ã‚¯ãŒå‡ºç¾ã™ã‚‹ã¾ã§å¾…ã¤æœ€å¤§æ™‚é–“
SEARCH_RETRY_WAIT_MS = 4000  # æ¤œç´¢çµæœãŒã€Œã‚‚ã†ä¸€åº¦æŠ¼ã™ã¨å‡ºã‚‹ã€å ´åˆã®è¿½åŠ å¾…æ©Ÿï¼ˆãƒŸãƒªç§’ï¼‰

# ãƒœãƒƒãƒˆå¯¾ç­–å›é¿: å®Ÿãƒ–ãƒ©ã‚¦ã‚¶ã«è¿‘ã„ User-Agent ã¨ãƒ˜ãƒƒãƒ€ãƒ¼
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
)
VIEWPORT = {"width": 1280, "height": 720}


def _composite_key(card_number: str, card_name: str) -> str:
    """é‡è¤‡ç”¨ã®è¤‡åˆã‚­ãƒ¼ï¼ˆJSON ã®ã‚­ãƒ¼ç”¨ï¼‰"""
    return f"{card_number}|{card_name}"


def get_card_entries() -> list[tuple[str, str, bool]]:
    """
    CSV ã‹ã‚‰ (card_number, card_name, is_duplicate) ã®ä¸€è¦§ã‚’å–å¾—
    is_duplicate: åŒã˜ card_number ã§è¤‡æ•°ã‚«ãƒ¼ãƒ‰åãŒã‚ã‚‹å ´åˆ True
    """
    if not CSV_PATH.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: {CSV_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    # (card_number, card_name) ã®é‡è¤‡ãªã—
    pairs: set[tuple[str, str]] = set()
    with open(CSV_PATH, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            cn = row.get("card_number", "").strip()
            name = (row.get("ã‚«ãƒ¼ãƒ‰å") or "").strip()
            if cn and name:
                pairs.add((cn, name))
    # card_number ã”ã¨ã®ã‚«ãƒ¼ãƒ‰åã®æ•°
    cn_counts = Counter(cn for cn, _ in pairs)
    return [(cn, name, cn_counts[cn] > 1) for cn, name in sorted(pairs)]


# ã‚µã‚¤ãƒˆãŒã€Œè¦‹ã¤ã‹ã‚‰ãªã„ã€ã¨æ˜ç¤ºã—ãŸå ´åˆã®æˆ»ã‚Šå€¤ï¼ˆURL ã§ã¯ãªã„ï¼‰
NOT_FOUND_ON_SITE = "NOT_FOUND"


def normalize_card_name_for_search(name: str) -> str:
    """
    æ¤œç´¢ç”¨ã«ã‚«ãƒ¼ãƒ‰åã‚’æ­£è¦åŒ–ã€‚
    - å…¨è§’ï¼†â†’åŠè§’&ï¼ˆåºƒå ´ã¯åŠè§’ã§ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰
    - æœ«å°¾ã® (SA) / ï¼ˆSAï¼‰ / (HR) ãªã©æ‹¬å¼§è¡¨è¨˜ã‚’é™¤å»
    """
    if not name or not name.strip():
        return name
    s = name.strip()
    # åºƒå ´ã¯ã€Œãƒ¬ã‚·ãƒ©ãƒ &ãƒªã‚¶ãƒ¼ãƒ‰ãƒ³GXã€ãªã®ã§ã€å…¨è§’ï¼†ã‚’åŠè§’&ã«çµ±ä¸€
    s = s.replace("ï¼†", "&")
    # æœ«å°¾ã®åŠè§’ãƒ»å…¨è§’æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸè¡¨è¨˜ï¼ˆSA, HR, SR ãªã©ï¼‰ã‚’ç¹°ã‚Šè¿”ã—é™¤å»
    while True:
        m = re.search(r"\s*[(\ï¼ˆ](SA|HR|SR|SAR|MUR|UR|CSR|SSR|P|PR)[)\ï¼‰]\s*$", s, re.IGNORECASE)
        if not m:
            break
        s = s[: m.start()].rstrip()
    return s


def _do_search_and_parse(page, query: str, card_number: str, try_click_search_retry: bool = False) -> tuple[str | None, bool]:
    """
    æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€HTML ã‹ã‚‰è©²å½“ card_number ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    try_click_search_retry: True ã®ã¨ãã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ğŸ”æ¤œç´¢ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§å†æ¤œç´¢ã‚’è©¦ã™ã€‚
    æˆ»ã‚Šå€¤: (URL ã¾ãŸã¯ None, ã‚µã‚¤ãƒˆå´ã§ NOT FOUND ã ã£ãŸã‹)
    """
    url = f"{SEARCH_URL}?s={query}"
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=20000)
        page.wait_for_timeout(int(PAGE_LOAD_WAIT_SEC * 1000))
        try:
            page.wait_for_selector(
                'a[href^="/"][href*="-"], a[href*="pokeca-chart.com/"][href*="-"]',
                timeout=RESULTS_LINK_WAIT_MS,
            )
        except Exception:
            pass
        page.wait_for_timeout(500)
        html = page.content()
    except Exception:
        return None, False

    def _parse(html_text: str) -> str | None:
        sp = BeautifulSoup(html_text, "html.parser")
        exp_slug = card_number.replace("/", "-").lower()
        for a in sp.find_all("a", href=True):
            h = a["href"].strip()
            if h.startswith("//"):
                h = "https:" + h
            elif h.startswith("/"):
                h = BASE_URL + h
            m = CARD_LINK_PATTERN_STD.match(h)
            if m:
                parts = m.group(1).split("-")
                num_part = parts[-2] + "/" + parts[-1]
                if num_part == card_number:
                    return h.rstrip("/") + "/"
            if "/" in card_number and any(c.isalpha() for c in card_number.split("/")[-1]):
                m2 = CARD_LINK_PATTERN_SPECIAL.match(h)
                if m2 and m2.group(1) == exp_slug:
                    return h.rstrip("/") + "/"
        return None

    found = _parse(html)
    if found:
        return (found, False)

    # ã‚µã‚¤ãƒˆãŒã€Œæ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚‚ã†ä¸€åº¦æŠ¼ã™ã¨å‡ºã¦ãã‚‹ã€ã‚ˆã†ã«é…å»¶è¡¨ç¤ºã—ã¦ã„ã‚‹å ´åˆã®ãƒªãƒˆãƒ©ã‚¤
    page.wait_for_timeout(SEARCH_RETRY_WAIT_MS)
    html2 = page.content()
    found = _parse(html2)
    if found:
        return (found, False)

    # åå‰ã®ã¿æ¤œç´¢ã§ã¾ã è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€æ–‡å­—ã¯å¤‰ãˆãšğŸ”æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦å†æ¤œç´¢
    if try_click_search_retry:
        for sel in ['input[type="submit"]', 'button[type="submit"]', 'button:has-text("æ¤œç´¢")', '[aria-label="æ¤œç´¢"]']:
            try:
                page.locator(sel).first.click(timeout=2000)
                break
            except Exception:
                continue
        page.wait_for_timeout(SEARCH_RETRY_WAIT_MS)
        html3 = page.content()
        found = _parse(html3)
        if found:
            return (found, False)
        not_found = "NOT FOUND" in html.upper() or "NOT FOUND" in html2.upper() or "NOT FOUND" in html3.upper()
    else:
        not_found = "NOT FOUND" in html.upper() or "NOT FOUND" in html2.upper()
    return (None, not_found)


def search_and_extract_link(card_number: str, page, card_name: str | None = None) -> str | None:
    """
    pokeca-chart.com ã§æ¤œç´¢ã—ã€ã‚«ãƒ¼ãƒ‰è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡ºã€‚
    æ¤œç´¢ã¯ã€Œå‹ç•ª åå‰ã€â†’ã€Œåå‰ã®ã¿ã€ã®é †ã€‚åå‰ã®ã¿ã§è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ğŸ”æ¤œç´¢ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§å†æ¤œç´¢ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: URL | NOT_FOUND_ON_SITE | None
    """
    if card_name:
        search_name = normalize_card_name_for_search(card_name)
        # å‹ç•ª åå‰ â†’ åå‰ã®ã¿ ã®é †ã€‚åå‰ã®ã¿ã®ã¨ãã¯è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°æ¤œç´¢ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§å†æ¤œç´¢
        queries = [
            (f"{card_number} {search_name}", False),
            (search_name, True),  # åå‰ã ã‘ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ğŸ”ã‚’æŠ¼ã—ã¦å†æ¤œç´¢ï¼‰
        ]
    else:
        queries = [(card_number.replace("/", "%2F"), False)]

    for q, click_retry in queries:
        # & ã‚’ãã®ã¾ã¾ã«ã™ã‚‹ã¨ URL ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒºåˆ‡ã‚Šã¨è§£é‡ˆã•ã‚Œã€Œãƒ•ã‚¡ã‚¤ãƒ¤ãƒ¼ã€ã ã‘é€ã‚‰ã‚Œã‚‹ã®ã§å¿…ãš quote
        query = quote(q) if (" " in q or "&" in q) else q
        found, not_found = _do_search_and_parse(page, query, card_number, try_click_search_retry=click_retry)
        if found:
            return found
        if not_found:
            continue  # æ¬¡ã®ã‚¯ã‚¨ãƒªã‚’è©¦ã™
    # æœ€å¾Œã®è©¦è¡Œã§ NOT FOUND ã ã£ãŸã‹ã¯åˆ¤å®šã—ãªã„ï¼ˆè¤‡æ•°ã‚¯ã‚¨ãƒªè©¦ã—ãŸãŸã‚ï¼‰
    return NOT_FOUND_ON_SITE


def load_existing_links() -> dict[str, str]:
    """æ—¢å­˜ã® JSON ã‚’èª­ã¿è¾¼ã‚€"""
    if OUTPUT_PATH.exists():
        try:
            with open(OUTPUT_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {}


def main():
    entries = get_card_entries()
    if "--test" in sys.argv:
        entries = entries[:8]  # 055/050 ãªã©é‡è¤‡ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«å¤šã‚
        print("â€» ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: å…ˆé ­8ä»¶ã®ã¿")

    print(f"ã‚«ãƒ¼ãƒ‰æ•°: {len(entries)} ä»¶ï¼ˆé‡è¤‡å‹ç•ªã¯ã‚«ãƒ¼ãƒ‰åã”ã¨ã«å–å¾—ï¼‰")

    existing = load_existing_links()
    results: dict[str, str] = dict(existing)

    # ä»Šå›å®Ÿéš›ã«å‡¦ç†ã™ã‚‹ä»¶æ•°ï¼ˆæ—¢å­˜ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    to_process = [
        (card_number, card_name, is_duplicate)
        for card_number, card_name, is_duplicate in entries
        if (_composite_key(card_number, card_name) if is_duplicate else card_number) not in results
    ]
    total_to_process = len(to_process)
    if total_to_process < len(entries):
        print(f"æ—¢å­˜ã«ã‚ˆã‚Š {len(entries) - total_to_process} ä»¶ã‚¹ã‚­ãƒƒãƒ—ã€ä»Šå› {total_to_process} ä»¶ã‚’å‡¦ç†")

    fetched = 0
    use_headed = "--headed" in sys.argv  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¡¨ç¤ºã§ãƒœãƒƒãƒˆå¯¾ç­–ãŒå³ã—ã„å ´åˆã«è©¦ã™
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=not use_headed)
        context = browser.new_context(
            user_agent=USER_AGENT,
            viewport=VIEWPORT,
            locale="ja-JP",
            extra_http_headers={
                "Accept-Language": "ja,en;q=0.9",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            },
        )
        page = context.new_page()
        page.set_default_timeout(20000)

        for n, (card_number, card_name, is_duplicate) in enumerate(to_process, 1):
            key = _composite_key(card_number, card_name) if is_duplicate else card_number
            label = f"{card_number} {card_name}" if is_duplicate else card_number
            print(f"  [{n}/{total_to_process}] {label} ... ", end="", flush=True)
            result = search_and_extract_link(card_number, page, card_name if is_duplicate else None)
            if result and result != NOT_FOUND_ON_SITE:
                results[key] = result
                print(result)
                fetched += 1
            elif result == NOT_FOUND_ON_SITE:
                print("NOT FOUND")
            else:
                print("(è¦‹ã¤ã‹ã‚‰ãš)")
            time.sleep(REQUEST_DELAY_SEC)

        browser.close()

    # JSON ä¿å­˜ï¼ˆã‚­ãƒ¼ã§ã‚½ãƒ¼ãƒˆï¼‰
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(dict(sorted(results.items())), f, ensure_ascii=False, indent=2)

    print(f"\nå®Œäº†: {len(results)} ä»¶ã®ãƒªãƒ³ã‚¯ã‚’ä¿å­˜ï¼ˆæ–°è¦ {fetched} ä»¶ï¼‰")
    print(f"å‡ºåŠ›: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
